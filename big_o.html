<!DOCTYPE html>
<html>
<title>Complexity and Big-O</title>

<xmp theme="spacelab" style="display:none;">

# "Big-O" notation

One of the things a new programmer discoveres pretty quickly is that there are
often many different solutions to a single problem, and while they may achieve the
same result, they are not necessarily equally efficient at doing it. Some programmed solutions to a given problem are
faster or can be shown to use less memory, while others may be easier to understand and faster to actually implement.
Trade-offs are a pretty natural thing and present everywhere in this line of work, and often there is [no single best](https://en.wikipedia.org/wiki/No_free_lunch_theorem) data structure or algorithm to optimally cover all use cases, often even within a narrowly defined class of problems.

Nevertheless, we would still at least like to have the ability to categorize different algorithms in terms of these kinds of trade-offs, in order to aid in decision making when designing or rewriting software. [Complexity analysis](https://en.wikipedia.org/wiki/Analysis_of_algorithms) is the primary means for achieving this. Principal among the various complexity measures that can be used is what is called **Big-O** notation.

Big-O, at a general level, is a means of describing the limiting behavior of a mathematical function. In context of complexity analysis, it is a way of classifying, in very broad strokes, how a computer algorithm scales as a function of the size of its input data.

What do I mean by an algorithm in this case? I really mean any single piece of code within a program that you write. For example, consider the following method that takes in an integer array, and returns the largest value inside that array:

```java
public int max(int[] my_array) {
    int largest = my_array[0];

    for (int i = 0; i < my_array.length; i++) {
        if (my_array[i] > largest) {
            largest = my_array[i];
        }
    }

    return largest;
}
```

If we consider peeking inside an element of a data structure (our array) and comparing something or doing arithmetic with is as one unit of "work", how much "work" could running this method require, relative to the size of the array?

We see that we are actually visiting and doing a `comparison` with every single element in the array once. If we let `n` be the size of the input array, we might say that the `max` method generally requires `n` units of work in order to find the maximum value.

What about a method like this, that computers the sine of the square of the mid-point of an array (weird, but bear with me for a second):

```java
public double sinSquare(int[] x) {
    int n = x.length;
    double mid_value = x[n/2];
    return Math.sin(mid_value * mid_value);
}
```

How much work does this method do as a function of the size of the input array? If you think about it, though we are using the size of the array to find the mid-point, the total amount of work that we are doing does not really depend on it. A very nice thing about arrays is that they are *random-access* data structures, which means getting the first, second, middle, last, or *any* single element in an array all have the same computational cost, and it happens to be a very low cost. The `x.length` attribute is an integer that is stored when the array is first created, and is readily available (i.e. the elements of the array aren't being counted behinds the scene to get this number).

Since we are always doing a little math to locate and grab the middle element in the array, and visiting no other element regardless of the size of the array, the computational cost of this function is independent from its size. At least in the sense of conceptualizing its cost as the size grows. We are also doing a sequence of mathematical operations (squaring the number be find at the midpoint, then taking the sine of it), but none of that actually gets more or less expensive as the size of the input array changes.

Big-O is a tool meant to give us a more specific means to describe these things in a standard form. And when you get used to it, its actually pretty intuitive.

### Technical definition

These notes would be incomplete without the technical definition of Big-O. On the other hand, don't get too hung up on it just yet, as these notes are meant to be a less formal discussion overall.

>  We say that a function $f$ is in order $\mathcal{O}(g)$ if, for at least one choice of a constant $k > 0$, you can find a constant $a$ such that the inequality $0 \le f\left(x\right) \le k \cdot g\left(x\right)$ holds for all $x > a$.

This is a very mathematically precise way of saying "$f$ grows no quicker than the function $g$ as the size of its input grows arbitrarily large".

One thing to highlight out of this definition is the consequence that if $f$ is in class $\mathcal{O}(g)$ , then a constant multiple of it like $0.5 \cdot f$ is STILL in in the same class, $\mathcal{O}(g)$ . That isn't all that informative at the moment but will come up a bit later.

### Worst case vs. average case

Our default mode at looking at the complexity of functions is to represent a bound on the worst-case scenario for the behavior of a function with Big-O. However, if there is a good argument to be made that the worst-case scenario is a bit of a corner case and that, on average, you expect certain structure of your input data to influence the behavior of the function, this may be reported separately. This is sometimes difficult to do as it is a probabilistic argument, but it is an important distinction that can be made as we will see later in the course.

In other words, some times we will see "Worst case" and "Best case" reported for some common algorithms and data structure operations. For now, we will illustrate just the worst-case analysis.

### Linear complexity: $\mathcal{O}(n)$

I think the easiest example of Big-O to understand getting started is what we call the "linear" complexity class, or $\mathcal{O}(n)$. We actually already saw this in the first example above, the maximum-finding method:

```java
public int max(int[] my_array) {
    int largest = my_array[0];

    for (int i = 0; i < my_array.length; i++) {
        if (my_array[i] > largest) {
            largest = my_array[i];
        }
    }

    return largest;
}
```

How much work do we generally expect this to do, *as a function of the size of the input array*?
We fully expect to loop over and visit every element in the array (thats hard-coded into the method in fact), so the amount of work is exactly equal to the array size. So, we would call this $\mathcal{O}(n)$  where, again, n refers to the size of the input.

The key thing to get is that something can belong to a particular complexity class (like $\mathcal{O}(n)$ ) even if the amount of work *isn't* guaranteed to be *exactly* that much. For example, consider the task of finding the location of a specific value in an array:

```java
public int findIndex(char[] letters, char value) {
    // finds the first location of the char value within char array, letters
    // if not found in "letters", returns -1
    for (int i = 0; i < letters.length; i++) {
        if (letters[i].equals(value)) {
            return i;
        }
    }

    return -1
}
```

Again, how much work do we expect this to do, in terms of poking around in the input array `letters`?
This one might seem a bit different, since if we find the location of the value, we break out of the loop by returning its location without having to visit the entire thing. Unfortunately, there's nothing about the situation which can tell us how likely that is to happen.

But at the very least, we know we're looking at no more elements than the size itself - we know that we *MIGHT* visit the entire input array in order to find the value (or fail to find it altogether), though it might be less. Complexity analysis tends to lean towards the more pessimistic side. Since we only know an upper bound, would actually classify this as linear complexity, $\mathcal{O}()$ . If we knew something specific about how `letters` is sorted or other information like that (making it much more efficient than it would seem), it's possible that we might be able to classify it differently, but in the general case that we have, all we can say is that we are doing no more than `n` units of work as `n` grows larger (i.e., $\mathcal{O}(n)$ .

Lastly, consider the example of going over an array two times in a row (not a great design, just highlighting a point):

```java
public double Sum(double[] data) {
    // loop over it to print all numbers out
    for (number : data) {
        System.out.println(number);
    }

    //loop over it a second time to computer the sum of the array
    double sum = 0.0;
    for (number : data) {
        sum += number;
    }
}
```

We loop over and visit every element of the array once to print it, and then again to accumulate their values into a sum. Doing this as two separate loops instead of within a single loop is actually double the amount of work in accessing the elements of the array, but what is its complexity class? There are $2n$ access operations, however $\mathcal{O}(2n) = \mathcal{O}(n)$ , by definition. In other words, we still consider this to be a linear method on the input data, (rather than two-times-linear), the exact same complexity class if we more efficiently had put everything under a single for-loop. In other words, Big-O doesn't cover all nuances of design, as we'll see later in the course, its a tool for differentiating solutions from a larger birds-eye view.

### Constant complexity: $\mathcal{O}(1)$

In some sense, this is taking the discussion back a step back a bit from linear complexity, but $\mathcal{O}(1)$ (or what we call the "constant" complexity class),
is slightly trickier for some to understand at first. But $\mathcal{O}(1)$ is meant for classifying methods whose work has no dependence on the size of its input. This is the "nicest" complexity class you can have for something, as it implies perfect scalability.

As always, the devil may be in the details of course. An array algorithm that is $\mathcal{O}(1)$ may actually be extremely computationally expensive when run, but we can at least guarantee by being an $\mathcal{O}(1)$ algorithm is that this cost is not a function of the size of its input.

The second example in the introduction section is actually an example of $\mathcal{O}(1)$:

```java
public double sinSquare(int[] x) {
    int n = x.length;
    double mid_value = x[n/2];
    return Math.sin(mid_value * mid_value);
}
```

As discussed before, we are finding the location of the midpoint of the array, then squaring and taking the sine of the value that we find there. Since we are doing exactly this regardless of the size of the array, and accessing the computed midpoint of the array does not require us to visit any other location of the array, this is method has constant complexity, $\mathcal{O}(1)$.

The mathematical operations surrounding the access of then midpoint of the array **DO** have a real computational cost, but again this cost does not have anything to do with the size of the array, so these costs get swept under the Big-O rug so to speak.

It's important to understand that $\mathcal{O}(1)$ does NOT imply that we are only doing one computation or single access, it's the name of a general complexity class that covers ANY case where the cost does not scale with the input. To make another quick example:

```java
public double lastMinusFirst(double[] numbers) {
    // computes the difference between the last and first element of the array
    return numbers[numbers.length - 1] - numbers[0];
}
```

This computes the difference between the last and first elements of an array of doubles. Pretty simple, and requires only two access into the array, and a subtraction operation between them. The focus is on accesses to the array really, and the number of them that we are doing is *independent* of the size of the array, it is always exactly two. So, we would say that this is also an $\mathcal{O}(1)$ method (i.e. we WOULD NOT call it $\mathcal{O}(2)$ just because it has two accesses into the input data).

### Quadratic complexity, $\mathcal{O}(n^2)$

The quadratic complexity class $\mathcal{O}(n^2)$ covers algorithms whose cost grows as the square of the size of its input.
There's a few ways that an algorithm like this can naturally come about.
For example, let's say that I have an array of integers, and want to compute and print out the product of each one of them with every other integer in the same array.

So for example, if my input array is `{1,5,2,5}`, I want to produce the same data in the multiplication table:

|   | `1` | `5` | `2` | `5` |
|-------|---|---|---|---|
|   `1`   | 1 | 5 | 2 | 5 |
|   `5`   | 5 | 25 | 10 | 25 |
|   `2`   | 2 | 10 | 4 | 10 |
|   `5`   | 5 | 25 | 10 | 25 |

We could accomplish this by printing this information line-by-line with a two-level for-loop:

```java
public void printMult(int[] data) {
    for (int i=0; i < data.length; i++) {
        for (int k=0; k < data.length; k++) {
            System.out.println(data[i] * data[k]);
        }
    }
}
```

How much work is this doing, in a relative-number-of-array-accesses sense?
At the first level of the for loop, we are visiting each element of the `data` array.
Then, for *each single one of those*, we are accessing every element of the array in the second level of the loop below it. So the number of array accesses is the length squared, $n^2$, and we would say that this method is $\mathcal{O}(n^2)$.

Just as for the other complexity classes, we don't necessarily need to always have EXACTLY n^2 accesses to be in $\mathcal{O}(n^2)$. For example one thing that you'll notice is that the table above is somewhat redundant; since multiplication is commutative (i.e. $x\cdot y = y \cdot x$) we really only need to top half or bottom half of it to have all the product information:

|   | `1` | `5` | `2` | `5` |
|-------|---|---|---|---|
|   `1`   | 1 | 5  | 2  | 5 |
|   `5`   |   | 25 | 10 | 25 |
|   `2`   |   |    | 4  | 10 |
|   `5`   |   |    |    | 25 |

We can restrict our double-for loop to only produce the top portion of the table, by limiting the second level of the loop to only start where the first level is currently at in the array:

```java
public void printMult(int[] data) {
    for (int i=0; i < data.length; i++) {
        for (int k=i; k < data.length; k++) {
            System.out.println(data[i] * data[k]);
        }
    }
}
```
This is a better solution, since it is only half of the work! However, what would its complexity class be?
We have $\frac{1}{2} n^2$ operations now. But just as noted before, a constant proportional reduction in work does not actually change the complexity class, this solution is actually STILL an $\mathcal{O}(n^2)$ one. In fact if we could provably do 0.00001 % of the array accesses of something with a rockstar algorithm, that solution should be used, but technically that impact is not reflected in its Big-O classification; proportional improvements are necessarily swept under the rug.

### Cubic (and higher polynomial) complexity: $\mathcal{O}(n^3)$, $\mathcal{O}(n^4)$, ...

Higher order polynomial complexity classes come about in ways similar to what we've seen so far. Any time we see nested loops over the same data structure:

```java
    for (int i=0; i < data.length; i++) {
        for (int k=i; k < data.length; k++) {
            for (int j=k; j < data.length; j++) {
                ...
```

it's a signal of polynomial complexity. If our nested loop is 2 levels deep and
both levels are limited by the size of the same array, that entire nest is $\mathcal{O}(n^2)$ (which we saw in the last section). If it's 3 levels deep, then it would be $\mathcal{O}(n^3)$. And the same caveats apply.

To be honest, cubic and higher complexity will be very, very rare in the algorithms and data structure operations that we discuss in this course. For day-to-day computing tasks, $\mathcal{O}(n^2)$ is often seen as the ceiling to avoid and certainty not exceed if at all possible. In fact, there's a [whole blog](https://accidentallyquadratic.tumblr.com/) dedicated towards documenting cases where $\mathcal{O}(n^2)$ code is accidentally released in production software, where better solutions are known to exist.

### Logarithmic complexity: $\mathcal{O}(\log n)$

Logarithmic complexity tends to be one of the tougher ones for some people to understand at first in terms of counting operations at a high level, so it may be easier to begin with an example.

Let's say that we want to find the location (index) of a value within an array. We saw before that this was $\mathcal{O}(n)$ in the most general case. But let's make the case a bit more specific, let's say that we actually know beforehand that the input array that we are given is sorted, so all values in it are in increasing order.

How would this change our strategy for finding the location of a specific value? We could ignore this extra bit of info altogether, and stick with the $\mathcal{O}(n)$ solution of just checking every place in the array. But an interesting strategy exists if we know that information is sorted. To understand it, think of how you would look for a name inside of a phone book or word inside a dictionary:

- Open up to a random page
- Look at the values there.
    - If what I'm looking before comes before (in the alphabet) what I am seeing, I should jump to a page in the half of the book that comes before where I'm at
    - If instead it is larger, jump to a page in the larger half
- Repeat this process, keeping track of which portions of the book the value can't be in

For numbers within an array, the phone-book-search-like procedure would look like

```java
public int search(double[] sorted_data, double value) {
    // lowest and highest locations in the array where i might find the value
    int n = sorted_data.length;
    int low = 0;
    int high = n - 1;

    // start process in the middle
    int location = n / 2;
    double located = sorted_data[location];

    while (located != value) {
        if (value < located) { // if we're high, look in the left half of search space
            high = location;   // restrict search space
        }
        else {                 // if low, look in the right half of search space
            low = location;    // restrict search space
        }

        // if my two limits meet without finding the value, it isn't in the array
        if (low == high) {
            location = -1;
            break;
        }
    }

    return location;
}
```

How much work does something like this take? What is its complexity class?
The answer to this is sort of between the lines of the procedure itself. Note
that every step of the algorithm, we are splitting the remaining search space in two,
determining which half will remain, **and discarding the other half** for the remainder of
the search. So at every step, the size of the search space is shrinking proportionally by one half.

The real question then is, how much does this work grow as the original search space size, $n$ grows?
If I were to double the array size to $2n$, the first step of the algorithm would divide it in half, leaving me with $n$, and the algorithm would continue as before. In other words, doubling the size increased the number of operations by one.

If I doubled it again to $2 \cdot 2 n = 4n$, I would be adding two more iterations of the search procedure over the original size, $n$. If kept doubling and doubling it, $2^k n$, I would be adding $k$ additional steps to the search. Therefore, **the amount of work increases proportional to the logarithm of the size of the input**. This is the key feature of the logarithmic complexity class, $\mathcal{O}(\log n)$.

Every single algorithm that belongs to $\mathcal{O}(\log n)$ has a "divide and conquer" type logic within it. They all process an $n$ sized amount of data by subdividing it in some manner, and iteratively subdividing it further. Perhaps not by a half exactly, but by some proportional amount that refocuses and shrinks the size of the search or operation space.

How good is $\mathcal{O}(\log n)$ in comparison to the other complexity classes? It exists between the constant class $\mathcal{O}(1)$ and the linear class $\mathcal{O}(n)$. Logarithmic complexity algorithms tend to be considered very efficient.

### References for further reading

- [Big-O Algorithm Complexity Cheat Sheet](http://bigocheatsheet.com/)

<div id="footer"></div>
</xmp>

<script src="src/strapdown.js"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script>

<script src="src/jquery-3.2.1.min.js"></script>
<script src="src/footer.js"></script>
</html>